\section{Related Work}
Two of the more common solutions for interfacing FPGA based hardware accelerators are AXI bus-based solutions~\cite{vipin2014zycap, sadri2013energy, xillybus2018} for FPGA SoC systems (like in Xilinx Zynq) and PCle-based solutions~\cite{xillybus2018, vipin2014dyract, gong2014efficient, de2016fflink, jacobsen2015riffa} for stand-alone systems connected to a host CPU. 

\subsection{AXI bus-based Solutions} 
ZyCAP~\cite{vipin2014zycap} was developed as an open source soft DMA controller for high performance partial reconfiguration on the Xilinx Zynq. 
It provides two interfaces, an AXI-Lite interface connected to general purpose (GP) port and another AXI4 interface connected to the high performance (HP) port, and achieves a maximum throughput of 382 MB/s between the external memory and the partial reconfigurable region (PRR). 

An infrastructure to evaluate the data processing bandwidth and the energy consumption between the ARM processor sub-system and the DRAM on Zynq using the AXI HP/ACP interface was developed in~\cite{sadri2013energy}.
The implementation achieved a maximum full-duplex data processing bandwidth of over 1.6 GB/s for a single HP/ACP port, running at 125 MHz on a Zynq XC7Z020-1C device.

Xillybus~\cite{xillybus2018} is a commercial solution which provides a very simple abstraction of the AXI ACP interface for the Zynq device. 
It is consists of a loopback demo hardware design with a full Linux distribution running on the ARM processor. 
Xillybus can achieve a maximum data rate of around 300 MB/s running at 100 MHz on a Zynq-based ZedBoard. 

VectorBlox MXP~\cite{severance2013embedded} is a commercial soft vector processor overlay targetting Altera or Xilinx FPGAs via the Avalon or AXI interfaces. 
Data transfer is handled by a double-buffered vector scratchpad and a dedicated DMA engine communicating with the scalar host processor via the AXI HP port.
MXP can operate at a maximum frequency of 110 MHz on a Xilinx XC7Z020 device with 16 vector lanes. 
It demonstrates up to $1000\times$ speedup over MicroBlaze. 

\subsection{PCIe-based Solutions} 
Northwest Logic~\cite{nwlogic2018} and Xillybus~\cite{xillybus2018} are two representative commercial solutions which support PCIe interfaces for different generations while providing portability across different FPGA devices. 
Northwest Logic is licensed closed source while Xillybus is provided free of charge for research and teaching purposes. 
Xillybus users have full access to customize the complete Xillybus project, including the hardware design, the device driver and the software applications, with good documentation provided.
The latest version of Xillybus (Revision XL) can achieve a maximum data rate of around 3.5 GB/s for Gen2x8 and Gen3x4 PCIe interfaces.

In addition to these commercial solutions, there are a number of academic PCIe-based solutions, such as DyRACT~\cite{vipin2014dyract} and EPEE~\cite{gong2014efficient}.
DyRACT mainly focuses on dynamic partial reconfiguration over the PCIe Gen2x4 interface, along with a configuration controller and clock management, while EPEE was developed as a general purpose PCIe communication library, targeting a wide range of FPGA devices. 

RIFFA~\cite{jacobsen2015riffa} is an open source reusable framework for the integration of FPGA accelerators with workstations supporting PCIe Gen 2 and Gen3 protocols. 
A scatter-gather DMA-based design bridges the vendor-specific PCIe Endpoint core and multiple communication channels for user defined IP cores. 
The latest release of RIFFA (version 2.2.2) achieves a unidirectional maximum bandwidth of 3.6 GB/s for the Gen2x8 configuration on the Xilinx VC707 platform and 3.5 GB/s for the Gen3x4 configuration on the Terasic DE5-Net.

Other recent solutions include ffLink~\cite{de2016fflink} and JetStream~\cite{vesper2016jetstream}. 
ffLink is an open source PCIe Gen3x8 interface which achieves a throughput of 7.06 GB/s on a Xilinx VC709 platform. 
ffLink is based on the AXI infrastructure with Xilinx CDMA engines, limiting its use beyond Xilinx devices. 
JetStream~\cite{vesper2016jetstream} provides both FPGA-to-Host and FPGA-to-FPGA communication, and supports PCIe Gen3x8  with a peak unidirectional bandwidth of 7.05 GB/s. 
However, poor documentation and tool compatibility issues limit its use.

Table~\ref{theo_bw} shows the theoretical bandwidth of the AXI and PCIe memory interfaces. 
Although the total theoretical bandwidth of an AXI interface is comparable to that of a PCIe interface, many of the AXI-based solutions are not able to make full use of all available high performance HP/ACP ports, while the PCIe-based counterparts can support up to 8 lanes and some of them achieve a bandwidth which is close to the theoretical maximum. 
Among the existing implementations, Xillybus and RIFFA appear to be the most promising solutions due to their availability, ease-of-use and portability. 
In subsequent sections we develop high performance overlay-based accelerators designs using Xillybus and RIFFA.

\begin{table}[tb]
	%	\renewcommand{\arraystretch}{1.2}
	\caption{Theoretical bandwidth of typical memory interfaces.}
	\label{theo_bw}
	\centering
%	\small
	\resizebox{\columnwidth}{!}{
		\begin{threeparttable}
			\begin{tabular}{lrrrcr}
				\toprule
				Interface & Frequency & Upstream BW & Downstream BW & Ports/Lanes & Total BW\tnote{1} \\ \midrule
				AXI HP    &   150 MHz &   1200 MB/s &     1200 MB/s &      4      &         9600 MB/s \\
				AXI ACP   &   150 MHz &   1200 MB/s &     1200 MB/s &      1      &         2400 MB/s \\
				PCIe Gen2 &   250 MHz &    500 MB/s &      500 MB/s &      8      &         8000 MB/s \\
				PCIe Gen3 &   250 MHz &    984 MB/s &      984 MB/s &      8      &        15800 MB/s \\ \bottomrule
			\end{tabular} 
			\begin{tablenotes}
				\item[1] In a streaming interface, the throughput is limited by either the upstream or downstream BW, depending on the number of inputs or outputs, and so the maximum theoretical throughput would be half of the total bandwidth reported here.
			\end{tablenotes}
		\end{threeparttable}
	}
\end{table}