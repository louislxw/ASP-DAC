\section{Related Work}
%Two of the more common solutions for interfacing FPGA based hardware accelerators are AXI bus-based solutions~\cite{vipin2014zycap, sadri2013energy, xillybus2018} for FPGA SoC systems (like in Xilinx Zynq) and PCle-based solutions~\cite{xillybus2018, vipin2014dyract, gong2014efficient, de2016fflink, jacobsen2015riffa} for stand-alone systems connected to a host CPU. 

%Typical PCIe-based solutions for interfacing FPGA based hardware accelerators can be found from~\cite{xillybus2018, vipin2014dyract, gong2014efficient, de2016fflink, jacobsen2015riffa}. Most of them achieves high bandwidth close to theoretical maximum.  

\subsection{Coarse-grained TM Overlays}
A number of overlays have been proposed which share functional units among kernel operations in an attempt to reduce overlay resource requirements~\cite{severance2013embedded, liu2015quickdough, taras2019impact}.
This time-multiplexing of the overlay means that it can change its behavior on a cycle-by-cycle basis while the compute kernel is executing, thus allowing sharing of the limited FPGA resources.


ADRES~\cite{mei2003adres} appeared to be the first integration of a very long instruction word (VLIW) processor tightly-coupled with a coarse-grained reconfigurable matrix. 
Some of the functional units (FUs) and register files (RFs) are shared by the VLIW processor and the reconfigurable matrix. 
Thus, there is no need to transfer the data between the processor and the reconfigurable array compared to the tranditional CGRAs. 
Taras~\cite{taras2019impact} implemented the ADRES as overlays on Intel and Xilinx FPGAs using CGRA-ME modeling framework~\cite{chin2017cgra}. 
The ADRES overlay can achieve up to $1.6\times$ speedup on Fmax and a reduction of 41\% LUT usage, with the FPGA architecture optimizations. 


QuickDough~\cite{liu2015quickdough} was proposed to address FPGA design productivity issues. 
It is comprised of an array of nearest neighbor interconnected processing elements (PEs), referred to as SCGRA overlay.  
Application specific SCGRA overlay were implemented on Zynq, achieving a speedup of up to $9\times$ compared to the same application qunning on the Zynq ARM processor. 
The 250 MHz FU consists of an ALU, multiport data memory ($256\times$32 bits) and customizable depth instruction ROM (Supporting 72-bit instructions) resulting in significant BRAM utilization. 
Due to the excessive usage of BRAMs, it has limited scalability as a maximum implementation of $5\times$5 array can fit on Zynq, despite of the frequency degradation caused by the tight placement and route. 


VectorBlox MXP~\cite{severance2013embedded} is a commercial soft vector processor overlay targetting Altera or Xilinx FPGAs via the Avalon or AXI interfaces. 
Data transfer is handled by a double-buffered vector scratchpad and a dedicated DMA engine communicating with the scalar host processor via the AXI HP port. 
MXP can operate at a maximum frequency of 110 MHz on a Xilinx XC7Z020 device with 16 vector lanes. 
It demonstrates up to $1000\times$ speedup over MicroBlaze. 
The programming model is user-friendly as it is a combination of ANSI-C and VectorBlox C extensions.  
However, it is difficult to implement the applications which cannot be easily vectorized. 


\subsection{Accelerator Interfacing Frameworks} 
Two of the more common solutions for interfacing FPGA based hardware accelerators are AXI bus-based solutions~\cite{vipin2014zycap, sadri2013energy, xillybus2018} for FPGA SoC systems (like the Xilinx Zynq) and PCIe-based solutions~\cite{xillybus2018, vipin2014dyract, gong2014efficient, jacobsen2015riffa} for stand-alone systems connected to a host CPU. 

Infrastructure to evaluate the data processing bandwidth and the energy consumption between the ARM processor sub-system and the DRAM on Zynq using the AXI HP/ACP interface was developed in~\cite{sadri2013energy}.
The implementation achieved a maximum full-duplex data processing bandwidth of over 1.6 GB/s for a single HP/ACP port, running at 125 MHz on a Zynq XC7Z020-1C device.

ZyCAP~\cite{vipin2014zycap} was developed as an open source soft DMA controller for high performance partial reconfiguration on the Xilinx Zynq. 
It provides two interfaces, an AXI-Lite interface connected to general purpose (GP) port and another AXI4 interface connected to the high performance (HP) port, and achieves a maximum throughput of 382 MB/s between the external memory and the partial reconfigurable region (PRR). 

%Xillybus~\cite{xillybus2018} is a commercial solution which provides a very simple abstraction of the AXI ACP interface for the Zynq device. 
%It is consists of a loopback demo hardware design with a full Linux distribution running on the ARM processor. 
%Xillybus can achieve a maximum data rate of around 300 MB/s running at 100 MHz on a Zynq-based ZedBoard. 


%Northwest Logic~\cite{nwlogic2018} and 
Xillybus~\cite{xillybus2018} is a representative commercial solution which supports both AXI (baseline) and PCIe (Revision XL) interfaces while providing portability across different FPGA devices. 
%Northwest Logic is licensed closed source 
While Xillybus is provided free of charge for research and teaching purposes, users have full access to customize the complete Xillybus project, including the hardware design, the device driver and the software applications, with good documentation provided. 
The baseline Xillybus has a data rate of $\sim$300 MB/s running at 100 MHz on a Xilix Zynq, and a maximum data rate of $\sim$3.5 GB/s for PCIe Gen 2$\times$8 interfaces in Revision XL.

There are a number of academic PCIe interfacing solutions, such as DyRACT~\cite{vipin2014dyract} and EPEE~\cite{gong2014efficient}.

RIFFA~\cite{jacobsen2015riffa} is an open source reusable framework for the integration of FPGA accelerators with workstations supporting PCIe Gen 2 and Gen 3 standards. 
A scatter-gather DMA-based design bridges the vendor-specific PCIe Endpoint core and multiple communication channels for user defined IP cores. 
The latest release of RIFFA (version 2.2.2) achieves a unidirectional maximum bandwidth of 3.6 GB/s for the Gen2$\times$8 configuration on the Xilinx VC707 platform and 3.5 GB/s for the Gen3$\times$4 configuration on the Terasic DE5-Net.

DyRACT focuses on dynamic partial reconfiguration over the PCIe Gen2$\times$4 interface, along with a configuration controller and clock management, while EPEE was developed as a general purpose PCIe communication library, targeting a wide range of FPGA devices.
JetStream~\cite{vesper2016jetstream} is a PCIe Gen3 solution which supports not only FPGA-to-Host communication but also multiple FPGA boards interfaced together. 
%\todo[inline]{Also add a link to JetStream, which also supports multiple FPGA boards interfaced together.}

%Other recent solutions include ffLink~\cite{de2016fflink} and JetStream~\cite{vesper2016jetstream}. 
%ffLink is an open source PCIe Gen3$\times$8 interface which achieves a throughput of 7.06 GB/s on a Xilinx VC709 platform. 
%ffLink is based on the AXI infrastructure with Xilinx CDMA engines, limiting its use beyond Xilinx devices. 
%JetStream~\cite{vesper2016jetstream} provides both FPGA-to-Host and FPGA-to-FPGA communication, and supports PCIe Gen3$\times$8  with a peak unidirectional bandwidth of 7.05 GB/s. 
%However, poor documentation and tool compatibility issues limit its use.


Though overlays have shown potential in improving FPGA design productivity, few of them have been developed as full accelerator systems except. 
%The PCIe-based memory soluctions provides a perfect bridge between the host processor and the FPGA-based accelerators, for their high bandwidth and low latency. 
Most of the FPGA SoC designs use AXI interface as the bridge between the embedded ARM (or soft MicroBlaze/NIOS) processor and the programmable logic. 
However, PCIe interfaces are used in larger systems where communication between a host PC and accelerator card is needed. 
Table~\ref{theo_bw} shows the theoretical bandwidth of the AXI and PCIe interfaces. 
Although the total theoretical bandwidth of an AXI interface is comparable to that of a PCIe interface, many of the AXI bus-based solutions are not able to make full use of all available high performance HP/ACP ports, while the PCIe-based counterparts can support up to 8 lanes and some of them achieve a bandwidth which is close to the theoretical maximum. 
Among the existing implementations, Xillybus, RIFFA and DyRACT appear to be the most promising solutions due to their availability, ease-of-use and portability. 
In subsequent sections we develop high performance overlay-based accelerators designs using these solutions and analyse the overlay capabilities with different interfaces.


%\begin{comment}
\begin{table}
	%	\renewcommand{\arraystretch}{1.2}
	\caption{Theoretical bandwidth of typical memory interfaces.}
	\label{theo_bw}
	\centering
%	\small
	\resizebox{\columnwidth}{!}{
		\begin{threeparttable}
			\begin{tabular}{lrrrcr}
				\toprule
				Interface & Frequency & Upstream BW & Downstream BW & Ports/Lanes & Total BW\tnote{1} \\ \midrule
				AXI HP    &   150 MHz &   1200 MB/s &     1200 MB/s &      4      &         9600 MB/s \\
				AXI ACP   &   150 MHz &   1200 MB/s &     1200 MB/s &      1      &         2400 MB/s \\
				PCIe Gen2 &   250 MHz &    500 MB/s &      500 MB/s &      8      &         8000 MB/s \\
				PCIe Gen3 &   250 MHz &    984 MB/s &      984 MB/s &      8      &        15800 MB/s \\ \bottomrule
			\end{tabular} 
			\begin{tablenotes}
				\item[1] In a streaming interface, the throughput is limited by either the upstream or downstream BW, depending on the number of inputs or outputs, and so the maximum theoretical throughput would be half of the total bandwidth reported here.
			\end{tablenotes}
		\end{threeparttable}
	}
\end{table}
%\end{comment}