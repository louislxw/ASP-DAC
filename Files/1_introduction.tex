\section{Introduction}
Coarse-grained overlays, implemented on top of conventional fine-grained FPGAs, represent a promising solution to the design productivity challenge. 
Coarse-grained architectures enable faster compilation and software-like programmability, compared to the challange of increasingly complex applications being mapped to very fine grained resources.  
FPGA overlays can be broadly categorised as spatially configured (SC) or time-multiplexed (TM). 
In an SC overlay, a functional unit (FU) is allocated to a single computational operation in the accelerated kernel, with FUs connected by a routing network which is essentially static during kernel execution. 
A TM overlay, on the other hand, shares both the FUs and the interconnect across kernel operations, allowing improved usage of limited FPGA resources. 
However, TM overlays suffer from relatively large area overheads, due to their underlying processor-like architecture~\cite{severance2013embedded, rashid2014comparing, al2016fgpu, gray2016grvi, duarte2017scratch} or, for CGRA-like overlays, due to the routing resources and instruction storage requirements~\cite{paul2012remorph, liu2015quickdough}. 
Reducing the area overhead for CGRA-like overlays, specifically for the routing network, and utilizing the fast context switching capabilities is likely to result in improved usability with corresponding improvements in design productivity. 
%A linear time-multiplexed (TM) overlay~\cite{li2016area} has shown potential for use as an area efficient FPGA accelerator, and significant improvements to the architecture were described in~\cite{li2018time} which further improved the overlay performance.
In one design, a streaming architecture based on feed-forward pipelined datapaths, connected through a simple linear interconnect of lightweight functional units, was shown to drastically reduce interconnect requirements~\cite{li2018time}. 
However, performance analysis of such overlays often ignores the system interfacing aspects that are crucial to usability, as the interface between a main processor/memory subsystems and the overlay must be able to provide high-bandwidth (and large scale) data transmission.
Existing FPGA interfacing frameworks fall into one of two categories, AXI bus-based solutions for FPGA SoC systems (like the Xilinx Zynq) and PCle-based solutions for stand-alone systems connected to a host machine. 
Some of these frameworks abstract the interfaces with simple FIFO connections and provide streaming data transfers, which matches the requirements of our proposed linear TM overlay. 

In this paper, we implement an accelerator framework for  Xilinx Zynq-based FPGA systems using the AXI interface and for standalone FPGA systems with PCIe connectivity, based on the Xillybus~\cite{xillybus2018}, RIFFA~\cite{jacobsen2015riffa} and DyRACT~\cite{vipin2014dyract} communication frameworks. 
The architecture of a TM overlay and its essential control requirements are presented in detail. 
We present a full working implementation integrated with a memory subsystem, enabling practical usage. 
We evaluate the performance of the proposed AXI bus-based and PCIe-based overlay accelerators for a range of benchmarks. 
The proposed AXI bus-based accelerator is compared to a state-of-art VectorBlox MXP processor overlay~\cite{severance2013embedded}, in terms of throughput and resource usage. 
We compare the three proposed PCIe-based implementations and determine the most suitable interface in terms of supporting configuration of and streaming through the overlay. 
%The DyRACT-Overlay framework is open-source for future research and it is available at https://github.com/louislxw/linear\_tm\_overlay. 


%\begin{comment}
The main contributions of this paper can be summarized as follows:

\begin{itemize}
	\item	
	Examining memory interfaces for accelerator overlays, and provide a comprehensive analysis of embedded AXI and server PCIe interfaces for a range of benchmarks.
	
	\item
	A comparison with the state-of-the-art VectorBlox MXP processor overlay, showing 50\% throughput achieved using half of the bandwidth and less than 20\% of the hardware resource.
	
\end{itemize}
%\end{comment}
