\section{Experimental Evaluations}
We conducted experiments for both the AXI bus-based and PCIe-based overlay accelerators.  
For the AXI bus-based system, the experimental software runs on the ARM processor of the Xilinx ZedBoard. 
For the PCIe-based accelerators (implemented using PCIe-Xillybus, RIFFA and DyRACT respectively), the experiments are run on an HP Z420 workstation (six 3.5 GHz Intel Xeon E5-1650 cores) with a Xilinx VC707 evaluation board plugged into the PCIe Gen2 slot of the motherboard. 

\subsection{AXI-Xillybus System}
The AXI-Xillybus system is using one 32-bit wide AXI ACP port for data transmission, running at a frequency of 100 MHz. 
Hence the theoretical total bandwidth of this system is 800 MB/s, and the maximum streaming throughput is 400 MB/s in a full-duplex system. 

\subsubsection{AXI-Xillybus Loopback Test}
Before the accelerator system can be developed, we first ensure that the demonstration bundle provided by the Xillybus vendor operates correctly on our Zynq-based system. 
%The demonstration bundle includes an example FPGA design containing just a single FIFO used to loop the data back, along with a device driver and sample software code. 
We evaluate the performance of Xillybus by testing the round-trip data transmission throughput using the default settings, using sequential and parallel (multi-threading) programming for \textit{read} and \textit{write} functions respectively. 
%the write function and read function are executing sequentially by default, which means the read process has to wait until the write process has finished. 
%It is inefficient for a full-duplex interface to work in half-duplex mode, and so as to make full use of the Xillybus IP core, we adopt a multi-threading technique (using Pthreads) to improve its performance by creating two different threads for the write and read functions respectively. 

The bandwidth of the loopback test for AXI-Xillybus using different data block sizes is shown in Figure~\ref{axi_xillybus_loopback_bw}. 
As seen from this diagram, the single-threaded loopback test saturates at a throughput of around 150 MB/s (maximum of 160 MB/s) when the transfer size exceeds 8K words, while the multi-thread loopback test saturates (with a maximum throughput of 330 MB/s) when the transfer size exceeds 500K words. 
The two dashed lines indicate the theoretical throughput of the 32-bit AXI-ACP port running at 100 MHz.
The throughput of the multi-thread test surpasses that of the single-thread test for data block sizes exceeding 64K words, and for large block sizes achieves approximately twice the throughput. 
Creating Pthreads introduces a significant overhead for smaller transfer sizes, thus the multi-threading version is only applicable for large data transfers. 

\input{Figures_tex/axi_xillybus_loopback_bw}

\subsubsection{AXI-Xillybus based Overlay Accelerator}
As previously discussed, Xillybus provides multiple streaming device files and seekable devices files which have access to the memory array. 
%The accelerator and its data streams are shown in Figure~\ref{axi_xillybus_overlay}. 
In our design, two 32-bit streaming device files are instantiated for data acquisition and one 32-bit seekable device file is responsible for the instruction load and overlay setup. 
The performance of the proposed single DSP V3 overlay~\cite{li2018time} based accelerator (referred to as AXI-Xillybus-Overlay) is evaluated and compared to VectorBlox MXP~\cite{severance2013embedded}, in terms of bandwidth and area consumption. To perform this comparison, we use the set of kernels shown in Table~\ref{benchmarks_system}, which are extracted from compute intensive applications available from~\cite{gopalakrishnan2007finding, hoy2015performance}. 
Both AXI-Xillybus-Overlay and MXP are running on Xillinux-2.0, which is the latest Linux distribution released for ZedBoard (kernel 4.4), with AXI-Xillybus using the Pthread multi-threading technique with a data block transfer size of 1M words.

\begin{table}[tb]
%	\renewcommand{\arraystretch}{1.2}
	\caption{DFG characteristics of benchmark set.}
	\label{benchmarks_system}
	\centering
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{clccccc}
		\toprule
		    & Benchmark &  \multicolumn{5}{c}{Characteristics}  \\ \cline{3-7}
		No. & Name      &  I/O  & graph &  op   & graph & graph \\
		    &           & nodes & edges & nodes & depth & width \\ \midrule
		1.  & chebyshev &  1/1  &  12   &   7   &   7   &   1   \\
		2.  & mibench   &  3/1  &  22   &  13   &   6   &   3   \\
		3.  & qspline   &  7/1  &  50   &  25   &   8   &   6   \\
		4.  & fft       &  6/4  &  24   &  10   &   3   &   4   \\
		5.  & kmeans    & 16/1  &  39   &  23   &   5   &   8   \\
		6.  & mm        & 16/1  &  31   &  15   &   4   &   8   \\
		7.  & spmv      & 16/2  &  30   &  14   &   3   &   8   \\
		8.  & stencil   & 15/2  &  30   &  14   &   5   &   6   \\ \bottomrule
	\end{tabular}
	}
\end{table}

Figure~\ref{axi_xillybus_mxp_bw} shows the performance of a single 32-bit linear TM overlay (AXI-Xillybus-Overlay) and a 16-lane 32-bit MXP (MXP-V16) implemented on a ZedBoard for the benchmarks given in Table~\ref{benchmarks_system}. 
As seen from the figure, MXP-V16 outperforms AXI-Xillybus-Overlay over all benchmarks (with the exception of `chebyshev'). 
Table~\ref{xillybus_mxp_area} shows the hardware resource usage of AXI-Xillybus-Overlay and MXP-V16. 
As seen from this table, AXI-Xillybus-Overlay is much more area efficient, and consumes only 19.8\% LUTs, 26.1\% FFs, 7.6\% BRAMs and 7.1\% DSP blocks, compared to MXP-V16. 

\input{Figures_tex/axi_xillybus_mxp_bw}

The MXP memory system is based on one 64-bit AXI HP port running at a frequency of 110 MHz while AXI-Xillybus is based on a single 32-bit AXI ACP port running at a frequency of 100 MHz. 
This corresponds to a theoretical maximum throughput of 880 MB/s for MXP-V16 and 400 MB/s for AXI-Xillybus-Overlay. 
As seen from Figure~\ref{axi_xillybus_mxp_bw}, MXP-V16 achieves a throughput of 460.6 MB/s (52.3\% of theoretical maximum) on average while AXI-Xillybus has a throughput of 226.6 MB/s (56.6\% of theoretical maximum) on average. 
%Figure~\ref{axi_xillybus_mxp_bw} indicates that AXI-Xillybus-V3 has a worse performance for the benchmarks with a larger number of I/O nodes. 
The throughput of AXI-Xillybus is about half of that of MXP-V16, mainly due to AXI-Xillybus using only 32-bit of the available 64-bit ACP port while MXP-V16 uses the full 64-bit HP port. 


\begin{table}[tb]
%	\renewcommand{\arraystretch}{1.2}
	\caption{Area overhead of AXI bus-based systems.}
	\label{xillybus_mxp_area}
	\centering
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{lrrrr}
		\toprule
		System               &                \multicolumn{4}{c}{Resource Usage}                \\ \cline{2-5}
		                     &            LUTs &              FFs &        BRAMs &         DSPs \\ \midrule
		AXI-Xillybus-Overlay &           5,747 &            5,798 &            5 &            8 \\
		MXP-V16              &          28,974 &           22,174 &           66 &          112 \\
		\textbf{Available}   & \textbf{53,200} & \textbf{106,400} & \textbf{140} & \textbf{220} \\ \bottomrule
	\end{tabular}
	}
\end{table}

Although the AXI bus-based Xillybus provides an area efficient interface solution for the Xilinx Zynq SoC, the throughput is still far below the theoretical maximum shown in Table~\ref{theo_bw}. 
This is mainly due to not making good use of the available HP/ACP ports (Xillybus uses only one 32-bit port which is operating at a frequency of 66.7\% of the theoretical maximum that the AXI bus on Zynq can support). 


\subsection{PCIe System}
Our PCIe system is based on the 8-lane Gen2 PCIe interface running at a frequency of 250 MHz, which corresponds to a maximum bandwidth of 8000 MB/s and hence a maximum streaming throughput of 4000 MB/s for a full-duplex system. 

\subsubsection{PCIe-Xillybus Loopback Test}
Xillybus has been upgraded to a new version (PCIe-Xillybus) to support high performance PCIe interface communication since 2015. 
The block diagram of PCIe-Xillybus for data loopback is almost the same as that of AXI-Xillybus, except that the 32-bit FIFO is replaced with a 128-bit FIFO and module \textit{Xillybus.v} (including the Xillybus IP core and a Xilinx 7 series integrated block for PCIe) is connected to an HP Z420 workstation via the PCIe link. 
Similar to AXI-Xillybus, we developed two host programs to evaluate the round-trip bandwidth in both half-duplex and full-duplex mode. 

The bandwidth of the loopback test for PCIe-Xillybus using different data block sizes is shown in Figure~\ref{pcie_xillybus_riffa_loopback_bw}. 
As seen from this diagram, the data transmission is very slow when the transfer size is less than 32K words. 
This is due to large system overhead for a data buffer size of less than 128 KBytes~\cite{xillybus2018}. 
The single-threaded loopback test saturates at a throughput of 1900 MB/s when the transfer size exceeds 128K words, while the multi-thread loopback test saturates (with a maximum throughput of 2300 MB/s) when the transfer size exceeds 500K words. 
%The single-threaded loopback test saturates at a throughput of around 1900 MB/s (maximum of 2000 MB/s) when the transfer size exceeds 128K words, while the multi-thread loopback test saturates (with a maximum throughput of 2300 MB/s) when the transfer size exceeds 500K words. 
The two dashed lines indicate the theoretical bandwidth of the Gen2x8 PCIe interface running at 250 MHz.
The throughput of the multi-thread test surpasses that of the single-thread test for data block sizes exceeding 256K words, and achieves around 1.2$\times$ higher bandwidth when transferring 1M words. 
Although the throughput of the multi-thread test is not as expected (twice that of the single-thread test), it shows better performance for the transmission of large data blocks. 

\input{Figures_tex/pcie_xillybus_riffa_loopback_bw}

\subsubsection{RIFFA Loopback Test}
Similarly, a loopback design using RIFFA was also implemented. 
A first word fall through (FWFT) FIFO is used for data loop back, and the RIFFA IP core is acting as the bridge between the vendor-specific PCIe Endpoint and the FWFT FIFO. 
Data transmission is under the control of a finite state machine (FSM), which executes the upstream transfers and downstream transfers in different processing states.  
Thus, it is not possible to achieve full-duplex data transmission. 

The bandwidth for RIFFA data loopback is shown in Figure~\ref{pcie_xillybus_riffa_loopback_bw}. 
As seen from this diagram, both the upstream transfers (read) and downstream transfers (write) saturate at around 2.85 GB/s when the transfer size is equal to 128K sample words, corresponding a total bandwidth of 2.85 GB/s, as read and write cannot happen simultaneously. 
The bandwidth degrades slightly when the transfer size exceeds 128k words due to the overutilization of BRAMs.
As mentioned previously, the RIFFA hardware was not developed for full-duplex data transmission. 
Hence the \textit{fpga\_send} function and \textit{fpga\_recv} function will execute sequentially and it is useless to use multi-threading techniques as was done for Xillybus. 

\subsubsection{PCIe-Xillybus based Overlay Accelerator}
%Figure~\ref{pcie_xillybus_overlay} shows the PCIe-Xillybus based overlay accelerator and its data streams. 
In the PCIe-Xillybus based overlay accelerator system, we propose replicating four 32-bit linear TM overlays, interfacing with the 128-bit FIFOs to make full use of the data width.
Thus, two 128-bit streaming device files are instantiated for data acquisition and one 128-bit seekable device file is responsible for instruction load and overlay setup. 

\subsubsection{RIFFA based Overlay Accelerator}
Integration of the linear TM overlay with RIFFA is quite similar to that of Xillybus, except that the standard FIFOs are replaced with FWFT FIFOs.  
%Four 32-bit overlays are connected with the 128-bit FWFT FIFOs to make full use of the data width. 
Although RIFFA has no access to the FPGA RAMs directly, it is possible to load the instructions and registers by adding a new state to the FSM control. 
The RX (view from the integrated overlay side) is divided into two processes: one for overlay initialization, and the other one for data transmission. 

Figure~\ref{pcie_xillybus_riffa_bw} shows the performance of four 32-bit linear TM overlays integrated with PCIe-Xillybus (PCIe-Xillybus-Overlay) and RIFFA (RIFFA-Overlay), respectively. 
AXI-Xillybus uses a block size of 1M words with the Pthread multi-threading technique, while RIFFA uses a block size of 128K words.
Both PCIe-Xillybus-Overlay and RIFFA-Overlay are running on an Ubuntu 14.04.5 workstation (six 3.5 GHz Intel Xeon E5-1650 cores) with a Xilinx VC707 evaluation board plugged into the PCIe Gen2 slot of the motherboard. 
As can be seen from Figure~\ref{pcie_xillybus_riffa_bw}, RIFFA-Overlay achieves an average throughput of 1300 MB/s (32.5\% of theoretical maximum), which is around 3.6$\times$ better than PCIe-Xillybus-Overlay with an average throughput of 358 MB/s (9\% of theoretical maximum). 
The ratio of the throughput in Mega-operations per second (MOPS) is proportional to that of data processing throughput in MB/s. 

\input{Figures_tex/pcie_xillybus_riffa_bw}

Table~\ref{pcie_xillybus_riffa_area} shows the hardware resource usage of RIFFA-Overlay and PCIe-Xillybus-Overlay. 
RIFFA-Overlay consumes 15\% fewer LUTs, 49\% more FFs, 19$\times$ more BRAMs and the same DSP blocks compared to PCIe-Xillybus-Overlay. 
While the logic and DSP resources used are comparable, there is a big difference in the BRAM utilization.
This is due to the half-duplex mode of the RIFFA interface, resulting in a significant BRAM consumption to implement the two large depth FWFT FIFOs. 
Developing a full-duplex RIFFA based system would minimize the BRAM resource usage.
 
\begin{table}[tb]
%	\renewcommand{\arraystretch}{1.2}
	\caption{Area overhead of PCIe-based systems.}
	\label{pcie_xillybus_riffa_area}
	\centering
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{lrrrr}
		\toprule
		System                &                  \multicolumn{4}{c}{Resource Usage}                   \\ \cline{2-5}
		                      &             LUTs &              FFs &          BRAMs &           DSPs \\ \midrule
		PCIe-Xillybus-Overlay &           16,027 &           12,488 &           14.5 &             32 \\
		RIFFA-Overlay         &           13,657 &           18,581 &          289.5 &             32 \\
		DyRACT-Overlay        &           17,344 &           16,464 &             10 &             32 \\
		\textbf{Available}    & \textbf{303,600} & \textbf{607,200} & \textbf{1,030} & \textbf{2,800} \\ \bottomrule
	\end{tabular}
	}
\end{table}
